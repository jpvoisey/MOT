{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MOT Vehicle Testing\n",
    "### Introduction\n",
    "In the UK, cars older than 3 years are required to have annual tests (MOT test) for vehicle safety, roadworthiness and emissions.\n",
    "This ongoing project is a study of the MOT test results for cars and similar small vehicles in 2016.\n",
    "\n",
    "The intention is carry out some exploratory data analysis before investigating models to predict whether or not a vehicle will pass its MOT given its age, mileage, make, model, location etc..\n",
    "\n",
    "### Data Download\n",
    "The data for this project was obtained as follows:\n",
    "\n",
    "* MOT test results (downloaded 24/12/2017) https://data.gov.uk/dataset/anonymised_mot_test \n",
    "* Postcode lookup (downloaded 24/12/2017) https://data.gov.uk/dataset/national-statistics-postcode-lookup-uk\n",
    "* Shapefiles for UK postcode boundaries (downloaded 09/02/2018) http://www.opendoorlogistics.com/wp-content/uploads/Data/UK-postcode-boundaries-Jan-2015.zip\n",
    "\n",
    "## Data Pre-Processing\n",
    "All data in archives was extracted and placed in a \"data\" folder.\n",
    "\n",
    "The Shapefiles data did not require any further pre-processing.\n",
    "\n",
    "### MOT Results (2016)\n",
    "As all MOT test results for all vehicles are provided, this is a large file (3GB).\n",
    "\n",
    "The first three lines of the file are shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_id|vehicle_id|test_date|test_class_id|test_type|test_result|test_mileage|postcode_area|make|model|colour|fuel_type|cylinder_capacity|first_use_date\n",
      "\n",
      "1645480751|1374211238|2016-01-01|4|NT|P|117033|SM|VOLKSWAGEN|POLO|BLACK|PE|1600|2000-06-23\n",
      "\n",
      "1393462389|1153769898|2016-01-01|4|NT|P|99292|NE|VOLKSWAGEN|PASSAT|BLUE|DI|1968|2006-11-30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Raw data for 2016 (3GB)\n",
    "data_file = \"data/test_result_2016.txt\"\n",
    "\n",
    "with open(data_file, 'r') as data_input:\n",
    "    for lines in range(3):\n",
    "        line = data_input.readline()\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling & Splitting\n",
    "The MOT test data is stored in a pipe delimited text file. For practical purposes and to simulate a survey, a small sample (1.5%) of all car and small vehicle (test class, 4) tests was taken.\n",
    "\n",
    "This data was then split into four parts:\n",
    "* Training (75%)\n",
    "* Development for Hyperparameter tuning (10%)\n",
    "* Ensemble for stacking various models if required (5%)\n",
    "* Testing, which will put asise (in a vault!) to test the final model of each type. (10%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of samples taken was 534,557\n",
      "This was split as follows:\n",
      "  Training - 427,993\n",
      "  Development - 53,282\n",
      "  Ensemble - 26,548\n",
      "  Testing - 53,282\n"
     ]
    }
   ],
   "source": [
    "# Initialize counts\n",
    "sample_count = 0\n",
    "testing_development_count = 0\n",
    "ensemble_count = 0\n",
    "\n",
    "# Output files\n",
    "training_file = \"data/MOT_result_2016_training.csv\"\n",
    "development_file = \"data/MOT_result_2016_development.csv\"\n",
    "ensemble_file = \"data/MOT_result_2016_ensemble.csv\"\n",
    "testing_file = \"data/MOT_result_2016_testing.csv\"\n",
    "\n",
    "# Sample Ratio (very large file, so sample just 1.5%)\n",
    "sample = 0.015\n",
    "\n",
    "# Split Ratio (10% for development & testing, 5% for ensemble, 75% left for training)\n",
    "testing_development_split = 0.1\n",
    "ensemble_split = 0.05\n",
    "\n",
    "# Use write mode and headers for 1st dataframe only\n",
    "mode = 'w'\n",
    "header = True\n",
    "\n",
    "# Process the file in chunks\n",
    "chunksize = 10**5\n",
    "\n",
    "for chunk in pd.read_csv(data_file, sep='|', chunksize=chunksize, error_bad_lines=False, warn_bad_lines=False):\n",
    "    if mode == 'w': # 1st dataframe sampled, set random state\n",
    "        chunk = chunk.query('test_class_id == 4').sample(frac=sample, random_state = 21)\n",
    "    else:\n",
    "        chunk = chunk.query('test_class_id == 4').sample(frac=sample)\n",
    "    # Split data into training, development, ensemble & testing\n",
    "    chunk_len = len(chunk)\n",
    "    testing_development_len = int(chunk_len * testing_development_split)\n",
    "    ensemble_len = int(chunk_len * ensemble_split)\n",
    "    # Testing Data\n",
    "    start = 0\n",
    "    end = testing_development_len\n",
    "    chunk[start:end].to_csv(testing_file, index=False, mode=mode, header=header)\n",
    "    # Ensemble Data\n",
    "    start = end + 1\n",
    "    end = testing_development_len + ensemble_len\n",
    "    chunk[start:end].to_csv(ensemble_file, index=False, mode=mode, header=header)\n",
    "    # Development (hyperparameter tuning) Data\n",
    "    start = end + 1\n",
    "    end = 2 * testing_development_len + ensemble_len\n",
    "    chunk[start:end].to_csv(development_file, index=False, mode=mode, header=header)\n",
    "    # Training Data\n",
    "    start = end + 1\n",
    "    chunk[start:].to_csv(training_file, index=False, mode=mode, header=header)\n",
    "    # Set mode for writing CSV file to append for subsequent samples & don't rewrite headers\n",
    "    mode = 'a'\n",
    "    header = False\n",
    "    # Update counts\n",
    "    sample_count += chunk_len\n",
    "    testing_development_count += testing_development_len\n",
    "    ensemble_count += ensemble_len\n",
    "\n",
    "# Subtracting testing and development counts to get training counts\n",
    "training_count = sample_count - 2 * testing_development_count\n",
    "print('The number of samples taken was ' + \"{:,}\".format(sample_count))\n",
    "print('This was split as follows:')\n",
    "print('  Training - ' + \"{:,}\".format(training_count))\n",
    "print('  Development - ' + \"{:,}\".format(testing_development_count))\n",
    "print('  Ensemble - ' + \"{:,}\".format(ensemble_count))\n",
    "print('  Testing - ' + \"{:,}\".format(testing_development_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, a 1.5% sample still gives a large dataset, which is more than adequate for my purposes.\n",
    "\n",
    "#### Data Summary\n",
    "The processed data contains 14 features (none were dropped) as summarised below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "</style>  \n",
       "<table id=\"T_259f965c_1a45_11e8_9a88_f079598ef41d\" > \n",
       "<thead>    <tr> \n",
       "        <th class=\"blank level0\" ></th> \n",
       "        <th class=\"col_heading level0 col0\" >Column</th> \n",
       "        <th class=\"col_heading level0 col1\" >Type</th> \n",
       "        <th class=\"col_heading level0 col2\" >No. Null Values</th> \n",
       "        <th class=\"col_heading level0 col3\" >No. Unique Values</th> \n",
       "        <th class=\"col_heading level0 col4\" >Values</th> \n",
       "    </tr></thead> \n",
       "<tbody>    <tr> \n",
       "        <th id=\"T_259f965c_1a45_11e8_9a88_f079598ef41d\" class=\"row_heading level0 row0\" >0</th> \n",
       "        <td id=\"T_259f965c_1a45_11e8_9a88_f079598ef41drow0_col0\" class=\"data row0 col0\" >test_id</td> \n",
       "        <td id=\"T_259f965c_1a45_11e8_9a88_f079598ef41drow0_col1\" class=\"data row0 col1\" >int64</td> \n",
       "        <td id=\"T_259f965c_1a45_11e8_9a88_f079598ef41drow0_col2\" class=\"data row0 col2\" >0</td> \n",
       "        <td id=\"T_259f965c_1a45_11e8_9a88_f079598ef41drow0_col3\" class=\"data row0 col3\" >401068</td> \n",
       "        <td id=\"T_259f965c_1a45_11e8_9a88_f079598ef41drow0_col4\" class=\"data row0 col4\" >[635544879, 1135632171, 1821645967, 727411847, '...']</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_259f965c_1a45_11e8_9a88_f079598ef41d\" class=\"row_heading level0 row1\" >1</th> \n",
       "        <td id=\"T_259f965c_1a45_11e8_9a88_f079598ef41drow1_col0\" class=\"data row1 col0\" >vehicle_id</td> \n",
       "        <td id=\"T_259f965c_1a45_11e8_9a88_f079598ef41drow1_col1\" class=\"data row1 col1\" >int64</td> \n",
       "        <td id=\"T_259f965c_1a45_11e8_9a88_f079598ef41drow1_col2\" class=\"data row1 col2\" >0</td> \n",
       "        <td id=\"T_259f965c_1a45_11e8_9a88_f079598ef41drow1_col3\" class=\"data row1 col3\" >399891</td> \n",
       "        <td id=\"T_259f965c_1a45_11e8_9a88_f079598ef41drow1_col4\" class=\"data row1 col4\" >[1123718405, 259476332, 1196670034, 257196698, '...']</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_259f965c_1a45_11e8_9a88_f079598ef41d\" class=\"row_heading level0 row2\" >2</th> \n",
       "        <td id=\"T_259f965c_1a45_11e8_9a88_f079598ef41drow2_col0\" class=\"data row2 col0\" >test_date</td> \n",
       "        <td id=\"T_259f965c_1a45_11e8_9a88_f079598ef41drow2_col1\" class=\"data row2 col1\" >datetime64[ns]</td> \n",
       "        <td id=\"T_259f965c_1a45_11e8_9a88_f079598ef41drow2_col2\" class=\"data row2 col2\" >0</td> \n",
       "        <td id=\"T_259f965c_1a45_11e8_9a88_f079598ef41drow2_col3\" class=\"data row2 col3\" >366</td> \n",
       "        <td id=\"T_259f965c_1a45_11e8_9a88_f079598ef41drow2_col4\" class=\"data row2 col4\" >['04, Jan 2016', '04, Jan 2016', '03, Jan 2016', '04, Jan 2016', '...']</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_259f965c_1a45_11e8_9a88_f079598ef41d\" class=\"row_heading level0 row3\" >3</th> \n",
       "        <td id=\"T_259f965c_1a45_11e8_9a88_f079598ef41drow3_col0\" class=\"data row3 col0\" >test_class_id</td> \n",
       "        <td id=\"T_259f965c_1a45_11e8_9a88_f079598ef41drow3_col1\" class=\"data row3 col1\" >int64</td> \n",
       "        <td id=\"T_259f965c_1a45_11e8_9a88_f079598ef41drow3_col2\" class=\"data row3 col2\" >0</td> \n",
       "        <td id=\"T_259f965c_1a45_11e8_9a88_f079598ef41drow3_col3\" class=\"data row3 col3\" >1</td> \n",
       "        <td id=\"T_259f965c_1a45_11e8_9a88_f079598ef41drow3_col4\" class=\"data row3 col4\" >[4]</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_259f965c_1a45_11e8_9a88_f079598ef41d\" class=\"row_heading level0 row4\" >4</th> \n",
       "        <td id=\"T_259f965c_1a45_11e8_9a88_f079598ef41drow4_col0\" class=\"data row4 col0\" >test_type</td> \n",
       "        <td id=\"T_259f965c_1a45_11e8_9a88_f079598ef41drow4_col1\" class=\"data row4 col1\" >object</td> \n",
       "        <td id=\"T_259f965c_1a45_11e8_9a88_f079598ef41drow4_col2\" class=\"data row4 col2\" >0</td> \n",
       "        <td id=\"T_259f965c_1a45_11e8_9a88_f079598ef41drow4_col3\" class=\"data row4 col3\" >3</td> \n",
       "        <td id=\"T_259f965c_1a45_11e8_9a88_f079598ef41drow4_col4\" class=\"data row4 col4\" >['NT' 'RT' 'ES']</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_259f965c_1a45_11e8_9a88_f079598ef41d\" class=\"row_heading level0 row5\" >5</th> \n",
       "        <td id=\"T_259f965c_1a45_11e8_9a88_f079598ef41drow5_col0\" class=\"data row5 col0\" >test_result</td> \n",
       "        <td id=\"T_259f965c_1a45_11e8_9a88_f079598ef41drow5_col1\" class=\"data row5 col1\" >object</td> \n",
       "        <td id=\"T_259f965c_1a45_11e8_9a88_f079598ef41drow5_col2\" class=\"data row5 col2\" >0</td> \n",
       "        <td id=\"T_259f965c_1a45_11e8_9a88_f079598ef41drow5_col3\" class=\"data row5 col3\" >5</td> \n",
       "        <td id=\"T_259f965c_1a45_11e8_9a88_f079598ef41drow5_col4\" class=\"data row5 col4\" >['F' 'P' 'PRS' 'ABR' 'ABA']</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_259f965c_1a45_11e8_9a88_f079598ef41d\" class=\"row_heading level0 row6\" >6</th> \n",
       "        <td id=\"T_259f965c_1a45_11e8_9a88_f079598ef41drow6_col0\" class=\"data row6 col0\" >test_mileage</td> \n",
       "        <td id=\"T_259f965c_1a45_11e8_9a88_f079598ef41drow6_col1\" class=\"data row6 col1\" >float64</td> \n",
       "        <td id=\"T_259f965c_1a45_11e8_9a88_f079598ef41drow6_col2\" class=\"data row6 col2\" >3248</td> \n",
       "        <td id=\"T_259f965c_1a45_11e8_9a88_f079598ef41drow6_col3\" class=\"data row6 col3\" >149682</td> \n",
       "        <td id=\"T_259f965c_1a45_11e8_9a88_f079598ef41drow6_col4\" class=\"data row6 col4\" >[22263.0, 72386.0, 58479.0, 54344.0, '...']</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_259f965c_1a45_11e8_9a88_f079598ef41d\" class=\"row_heading level0 row7\" >7</th> \n",
       "        <td id=\"T_259f965c_1a45_11e8_9a88_f079598ef41drow7_col0\" class=\"data row7 col0\" >postcode_area</td> \n",
       "        <td id=\"T_259f965c_1a45_11e8_9a88_f079598ef41drow7_col1\" class=\"data row7 col1\" >object</td> \n",
       "        <td id=\"T_259f965c_1a45_11e8_9a88_f079598ef41drow7_col2\" class=\"data row7 col2\" >0</td> \n",
       "        <td id=\"T_259f965c_1a45_11e8_9a88_f079598ef41drow7_col3\" class=\"data row7 col3\" >119</td> \n",
       "        <td id=\"T_259f965c_1a45_11e8_9a88_f079598ef41drow7_col4\" class=\"data row7 col4\" >['BN', 'WS', 'ME', 'BA', '...']</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_259f965c_1a45_11e8_9a88_f079598ef41d\" class=\"row_heading level0 row8\" >8</th> \n",
       "        <td id=\"T_259f965c_1a45_11e8_9a88_f079598ef41drow8_col0\" class=\"data row8 col0\" >make</td> \n",
       "        <td id=\"T_259f965c_1a45_11e8_9a88_f079598ef41drow8_col1\" class=\"data row8 col1\" >object</td> \n",
       "        <td id=\"T_259f965c_1a45_11e8_9a88_f079598ef41drow8_col2\" class=\"data row8 col2\" >0</td> \n",
       "        <td id=\"T_259f965c_1a45_11e8_9a88_f079598ef41drow8_col3\" class=\"data row8 col3\" >429</td> \n",
       "        <td id=\"T_259f965c_1a45_11e8_9a88_f079598ef41drow8_col4\" class=\"data row8 col4\" >['SUZUKI', 'PEUGEOT', 'NISSAN', 'VAUXHALL', '...']</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_259f965c_1a45_11e8_9a88_f079598ef41d\" class=\"row_heading level0 row9\" >9</th> \n",
       "        <td id=\"T_259f965c_1a45_11e8_9a88_f079598ef41drow9_col0\" class=\"data row9 col0\" >model</td> \n",
       "        <td id=\"T_259f965c_1a45_11e8_9a88_f079598ef41drow9_col1\" class=\"data row9 col1\" >object</td> \n",
       "        <td id=\"T_259f965c_1a45_11e8_9a88_f079598ef41drow9_col2\" class=\"data row9 col2\" >0</td> \n",
       "        <td id=\"T_259f965c_1a45_11e8_9a88_f079598ef41drow9_col3\" class=\"data row9 col3\" >8511</td> \n",
       "        <td id=\"T_259f965c_1a45_11e8_9a88_f079598ef41drow9_col4\" class=\"data row9 col4\" >['SWIFT SZ3 DDIS', '3008', 'QASHQAI', 'CORSA', '...']</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_259f965c_1a45_11e8_9a88_f079598ef41d\" class=\"row_heading level0 row10\" >10</th> \n",
       "        <td id=\"T_259f965c_1a45_11e8_9a88_f079598ef41drow10_col0\" class=\"data row10 col0\" >colour</td> \n",
       "        <td id=\"T_259f965c_1a45_11e8_9a88_f079598ef41drow10_col1\" class=\"data row10 col1\" >object</td> \n",
       "        <td id=\"T_259f965c_1a45_11e8_9a88_f079598ef41drow10_col2\" class=\"data row10 col2\" >0</td> \n",
       "        <td id=\"T_259f965c_1a45_11e8_9a88_f079598ef41drow10_col3\" class=\"data row10 col3\" >19</td> \n",
       "        <td id=\"T_259f965c_1a45_11e8_9a88_f079598ef41drow10_col4\" class=\"data row10 col4\" >['BLUE', 'RED', 'BLUE', 'SILVER', '...']</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_259f965c_1a45_11e8_9a88_f079598ef41d\" class=\"row_heading level0 row11\" >11</th> \n",
       "        <td id=\"T_259f965c_1a45_11e8_9a88_f079598ef41drow11_col0\" class=\"data row11 col0\" >fuel_type</td> \n",
       "        <td id=\"T_259f965c_1a45_11e8_9a88_f079598ef41drow11_col1\" class=\"data row11 col1\" >object</td> \n",
       "        <td id=\"T_259f965c_1a45_11e8_9a88_f079598ef41drow11_col2\" class=\"data row11 col2\" >0</td> \n",
       "        <td id=\"T_259f965c_1a45_11e8_9a88_f079598ef41drow11_col3\" class=\"data row11 col3\" >10</td> \n",
       "        <td id=\"T_259f965c_1a45_11e8_9a88_f079598ef41drow11_col4\" class=\"data row11 col4\" >['DI' 'PE' 'HY' 'EL' 'LP' 'OT' 'ED' 'FC' 'GB' 'LN']</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_259f965c_1a45_11e8_9a88_f079598ef41d\" class=\"row_heading level0 row12\" >12</th> \n",
       "        <td id=\"T_259f965c_1a45_11e8_9a88_f079598ef41drow12_col0\" class=\"data row12 col0\" >cylinder_capacity</td> \n",
       "        <td id=\"T_259f965c_1a45_11e8_9a88_f079598ef41drow12_col1\" class=\"data row12 col1\" >float64</td> \n",
       "        <td id=\"T_259f965c_1a45_11e8_9a88_f079598ef41drow12_col2\" class=\"data row12 col2\" >414</td> \n",
       "        <td id=\"T_259f965c_1a45_11e8_9a88_f079598ef41drow12_col3\" class=\"data row12 col3\" >1211</td> \n",
       "        <td id=\"T_259f965c_1a45_11e8_9a88_f079598ef41drow12_col4\" class=\"data row12 col4\" >[1248.0, 1560.0, 1461.0, 1364.0, '...']</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_259f965c_1a45_11e8_9a88_f079598ef41d\" class=\"row_heading level0 row13\" >13</th> \n",
       "        <td id=\"T_259f965c_1a45_11e8_9a88_f079598ef41drow13_col0\" class=\"data row13 col0\" >first_use_date</td> \n",
       "        <td id=\"T_259f965c_1a45_11e8_9a88_f079598ef41drow13_col1\" class=\"data row13 col1\" >datetime64[ns]</td> \n",
       "        <td id=\"T_259f965c_1a45_11e8_9a88_f079598ef41drow13_col2\" class=\"data row13 col2\" >1</td> \n",
       "        <td id=\"T_259f965c_1a45_11e8_9a88_f079598ef41drow13_col3\" class=\"data row13 col3\" >10442</td> \n",
       "        <td id=\"T_259f965c_1a45_11e8_9a88_f079598ef41drow13_col4\" class=\"data row13 col4\" >['26, Sep 2013', '02, Mar 2010', '28, Jun 2010', '16, May 2007', '...']</td> \n",
       "    </tr></tbody> \n",
       "</table> "
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x16f96d7c048>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read training data\n",
    "training_df = pd.read_csv(training_file, parse_dates=['test_date', 'first_use_date'])\n",
    "\n",
    "# Empty dataframe to store data description\n",
    "meta_data = pd.DataFrame()\n",
    "\n",
    "# Loop over each column obtaining data descriptors\n",
    "for col in training_df.columns:\n",
    "    col_data = {}\n",
    "    col_data['Column'] = col\n",
    "    col_data['Type'] = training_df[col].dtype\n",
    "    no_unique = len(training_df[col].unique())\n",
    "    col_data['No. Unique Values'] = no_unique\n",
    "    no_null = training_df[col].isnull().sum()\n",
    "    col_data['No. Null Values'] = no_null\n",
    "    if no_unique < 11:\n",
    "        col_data['Values'] = training_df[col].unique()\n",
    "    elif col in ['test_date', 'first_use_date']:\n",
    "        col_data['Values'] = [d.strftime('%d, %b %Y') for d in training_df.loc[:3, col]] + ['...']\n",
    "    else:\n",
    "        col_data['Values'] = training_df.loc[:3, col].tolist() + ['...']\n",
    "    meta_data = meta_data.append(col_data, ignore_index=True)\n",
    "\n",
    "meta_data = meta_data[['Column','Type','No. Null Values', 'No. Unique Values', 'Values']]\n",
    "meta_data.style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Postcode Lookup\n",
    "The MOT test results only provide the postcode area (city or area level). e.g. CF = Cardiff.\n",
    "\n",
    "The Postcode Lookup file is much more detailed than is required, containing information about individual Postcodes (street level)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Postcode 1</th>\n",
       "      <th>Postcode 2</th>\n",
       "      <th>Postcode 3</th>\n",
       "      <th>Date Introduced</th>\n",
       "      <th>User Type</th>\n",
       "      <th>Easting</th>\n",
       "      <th>Northing</th>\n",
       "      <th>Positional Quality</th>\n",
       "      <th>County Code</th>\n",
       "      <th>County Name</th>\n",
       "      <th>...</th>\n",
       "      <th>Middle Super Output Area Code</th>\n",
       "      <th>Middle Super Output Area Name</th>\n",
       "      <th>Output Area Classification Code</th>\n",
       "      <th>Output Area Classification Name</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Spatial Accuracy</th>\n",
       "      <th>Last Uploaded</th>\n",
       "      <th>Location</th>\n",
       "      <th>Socrata ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S20 6RU</td>\n",
       "      <td>S20  6RU</td>\n",
       "      <td>S20 6RU</td>\n",
       "      <td>06-1997</td>\n",
       "      <td>0</td>\n",
       "      <td>441432.0</td>\n",
       "      <td>382852.0</td>\n",
       "      <td>1</td>\n",
       "      <td>E99999999</td>\n",
       "      <td>(pseudo) England (UA/MD/LB)</td>\n",
       "      <td>...</td>\n",
       "      <td>E02001671</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6B3</td>\n",
       "      <td>Suburbanites;Semi-detached suburbia;Semi-detac...</td>\n",
       "      <td>-1.379193</td>\n",
       "      <td>53.340953</td>\n",
       "      <td>Postcode Level</td>\n",
       "      <td>21/09/2017</td>\n",
       "      <td>(53.340953, -1.379193)</td>\n",
       "      <td>1311804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TW4 7BD</td>\n",
       "      <td>TW4  7BD</td>\n",
       "      <td>TW4 7BD</td>\n",
       "      <td>01-1980</td>\n",
       "      <td>0</td>\n",
       "      <td>512373.0</td>\n",
       "      <td>175453.0</td>\n",
       "      <td>1</td>\n",
       "      <td>E99999999</td>\n",
       "      <td>(pseudo) England (UA/MD/LB)</td>\n",
       "      <td>...</td>\n",
       "      <td>E02000541</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4B1</td>\n",
       "      <td>Multicultural metropolitans;Challenged Asian t...</td>\n",
       "      <td>-0.383652</td>\n",
       "      <td>51.466899</td>\n",
       "      <td>Postcode Level</td>\n",
       "      <td>21/09/2017</td>\n",
       "      <td>(51.466899, -0.383652)</td>\n",
       "      <td>1630253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GU513ZQ</td>\n",
       "      <td>GU51 3ZQ</td>\n",
       "      <td>GU51 3ZQ</td>\n",
       "      <td>05-2003</td>\n",
       "      <td>1</td>\n",
       "      <td>481569.0</td>\n",
       "      <td>155037.0</td>\n",
       "      <td>1</td>\n",
       "      <td>E10000014</td>\n",
       "      <td>Hampshire</td>\n",
       "      <td>...</td>\n",
       "      <td>E02004757</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6B4</td>\n",
       "      <td>Suburbanites;Semi-detached suburbia;Older work...</td>\n",
       "      <td>-0.831674</td>\n",
       "      <td>51.288637</td>\n",
       "      <td>Postcode Level</td>\n",
       "      <td>21/09/2017</td>\n",
       "      <td>(51.288637, -0.831674)</td>\n",
       "      <td>652126</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Postcode 1 Postcode 2 Postcode 3 Date Introduced  User Type   Easting  \\\n",
       "0    S20 6RU   S20  6RU    S20 6RU         06-1997          0  441432.0   \n",
       "1    TW4 7BD   TW4  7BD    TW4 7BD         01-1980          0  512373.0   \n",
       "2    GU513ZQ   GU51 3ZQ   GU51 3ZQ         05-2003          1  481569.0   \n",
       "\n",
       "   Northing  Positional Quality County Code                  County Name  \\\n",
       "0  382852.0                   1   E99999999  (pseudo) England (UA/MD/LB)   \n",
       "1  175453.0                   1   E99999999  (pseudo) England (UA/MD/LB)   \n",
       "2  155037.0                   1   E10000014                    Hampshire   \n",
       "\n",
       "     ...     Middle Super Output Area Code Middle Super Output Area Name  \\\n",
       "0    ...                         E02001671                           NaN   \n",
       "1    ...                         E02000541                           NaN   \n",
       "2    ...                         E02004757                           NaN   \n",
       "\n",
       "  Output Area Classification Code  \\\n",
       "0                             6B3   \n",
       "1                             4B1   \n",
       "2                             6B4   \n",
       "\n",
       "                     Output Area Classification Name Longitude   Latitude  \\\n",
       "0  Suburbanites;Semi-detached suburbia;Semi-detac... -1.379193  53.340953   \n",
       "1  Multicultural metropolitans;Challenged Asian t... -0.383652  51.466899   \n",
       "2  Suburbanites;Semi-detached suburbia;Older work... -0.831674  51.288637   \n",
       "\n",
       "  Spatial Accuracy Last Uploaded                Location Socrata ID  \n",
       "0   Postcode Level    21/09/2017  (53.340953, -1.379193)    1311804  \n",
       "1   Postcode Level    21/09/2017  (51.466899, -0.383652)    1630253  \n",
       "2   Postcode Level    21/09/2017  (51.288637, -0.831674)     652126  \n",
       "\n",
       "[3 rows x 36 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postcode_file = \"data/National_Statistics_Postcode_Lookup_UK.csv\"\n",
    "\n",
    "postcode_df = pd.read_csv(postcode_file)\n",
    "postcode_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Postcode Area Extraction\n",
    "The file was processed to extract the postcode areas and basic geographic location only.\n",
    "\n",
    "Postcodes in England only are split into regions, so a Country_Region column was created with regions for England, and Country for Wales & Scotland."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Local Authority Name</th>\n",
       "      <th>Country Name</th>\n",
       "      <th>Region Name</th>\n",
       "      <th>Country_Region</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Postcode Area</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AB</th>\n",
       "      <td>Aberdeen City</td>\n",
       "      <td>Scotland</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Scotland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AL</th>\n",
       "      <td>St Albans</td>\n",
       "      <td>England</td>\n",
       "      <td>East of England</td>\n",
       "      <td>East of England</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B</th>\n",
       "      <td>Birmingham</td>\n",
       "      <td>England</td>\n",
       "      <td>West Midlands</td>\n",
       "      <td>West Midlands</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BA</th>\n",
       "      <td>Bath and North East Somerset</td>\n",
       "      <td>England</td>\n",
       "      <td>South West</td>\n",
       "      <td>South West</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BB</th>\n",
       "      <td>Hyndburn</td>\n",
       "      <td>England</td>\n",
       "      <td>North West</td>\n",
       "      <td>North West</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BD</th>\n",
       "      <td>Bradford</td>\n",
       "      <td>England</td>\n",
       "      <td>Yorkshire and The Humber</td>\n",
       "      <td>Yorkshire and The Humber</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BH</th>\n",
       "      <td>Poole</td>\n",
       "      <td>England</td>\n",
       "      <td>South West</td>\n",
       "      <td>South West</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BL</th>\n",
       "      <td>Bolton</td>\n",
       "      <td>England</td>\n",
       "      <td>North West</td>\n",
       "      <td>North West</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BN</th>\n",
       "      <td>Lewes</td>\n",
       "      <td>England</td>\n",
       "      <td>South East</td>\n",
       "      <td>South East</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BR</th>\n",
       "      <td>Bromley</td>\n",
       "      <td>England</td>\n",
       "      <td>London</td>\n",
       "      <td>London</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Local Authority Name Country Name  \\\n",
       "Postcode Area                                              \n",
       "AB                            Aberdeen City     Scotland   \n",
       "AL                                St Albans      England   \n",
       "B                                Birmingham      England   \n",
       "BA             Bath and North East Somerset      England   \n",
       "BB                                 Hyndburn      England   \n",
       "BD                                 Bradford      England   \n",
       "BH                                    Poole      England   \n",
       "BL                                   Bolton      England   \n",
       "BN                                    Lewes      England   \n",
       "BR                                  Bromley      England   \n",
       "\n",
       "                            Region Name            Country_Region  \n",
       "Postcode Area                                                      \n",
       "AB                                  NaN                  Scotland  \n",
       "AL                      East of England           East of England  \n",
       "B                         West Midlands             West Midlands  \n",
       "BA                           South West                South West  \n",
       "BB                           North West                North West  \n",
       "BD             Yorkshire and The Humber  Yorkshire and The Humber  \n",
       "BH                           South West                South West  \n",
       "BL                           North West                North West  \n",
       "BN                           South East                South East  \n",
       "BR                               London                    London  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postcode_area_file = \"data/Postcode Area.csv\"\n",
    "\n",
    "# Extract 1st letters of postcode, which is the postcode area \n",
    "postcode_df['Postcode Area'] = (postcode_df['Postcode 3'].str.extract('([A-Z]+)', expand=True))\n",
    "\n",
    "# Group by postcode area, keeping only basic geographic location\n",
    "postcode_df = postcode_df.groupby(['Postcode Area']).first()[['Local Authority Name', 'Country Name', 'Region Name']]\n",
    "\n",
    "# Create Country_Region column\n",
    "postcode_df.loc[postcode_df['Country Name'] == 'England','Country_Region'] = postcode_df.loc[postcode_df['Country Name'] == 'England','Region Name']\n",
    "postcode_df.loc[postcode_df['Country Name'] != 'England','Country_Region'] = postcode_df.loc[postcode_df['Country Name'] != 'England','Country Name']\n",
    "\n",
    "# Write to file\n",
    "postcode_df.to_csv(postcode_area_file)\n",
    "postcode_df.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
